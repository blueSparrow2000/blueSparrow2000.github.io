---
layout: post
title: "ML background"  # 페이지 타이틀
post-order: 4                               # (내 커스텀 변수) 같은 카테고리 내 정렬 순서
comments: true
---

# Manifold hypothesis

'세상의 모든 데이터는 고차원 공간 안의 저차원 매니폴드 위에 존재한다'는 딥러닝의 근본적인 가정이다.
딥러닝은 데이터를 가장 잘 설명하는 곡면을 찾는다. 즉 데이터들이 어떤 구조를 가진 부드러운 곡면 위에 존재하므로 딥러닝을 사용하면 그 근서적인 구조를 데이터로부터 학습해 낼 수 있다는 것이다.
이 구조는 간단하고 저차원이어서 컴퓨터가 처리할 수 있다. 또한, 메니폴드는 연속적이기 때문에 데이터들을 보간(interpolation) 하여 그 구조를 근사적으로 만들어낼 수 있다.

예시로는 찌그러진 종이를 펴는 과정을 딥러닝에 자주 비유한다.
딥러닝에서 각 연산과 활성함수를 통해 찌그러진 manifold를 평면으로 펼쳐 데이터 클래스 간 경계를 찾기 편하게 만든다는 것이다.

<p/>
  
# Training
딥러닝 모델을 학습시키는 과정이다. 주로 forward pass, back propagation으로 이루어져 있으며, 모델을 테스트 할때는 forward pass만 수행하면 된다.
학습시에는 데이터로부터 표현을 학습하기 위해 예측과 개선을 반복한다.

Input을 모델에 넣어 예측값을 얻으면, 실제값 (레이블) 과 차이를 계산하고, 역전파 알고리즘으로 모델의 가중치를 조절한다.


# Back propagation
역전파라고도 하며, 예측값과 실제값의 차이를 계산하는 손실함수를 토대로 gradient를 계산한다.
딥러닝 모델의 가중치와 input 값, true value 를 알고 있으므로, 연쇄법칙(chain rule)을 이용하면 gradient를 구할 수 있다.

grad(loss,w_i) 를 구하면 각각의 가중치를 얼마나 더해야 가장 가파르게 최적의 가중치로 수렴할 수 있는지 알 수 있다.

# Overfitting
딥러닝 과정에서 오버피팅은 필수이다.
학습을 하면서 





