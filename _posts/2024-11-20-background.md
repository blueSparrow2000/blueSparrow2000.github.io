---
layout: post
title: "ML background"  # 페이지 타이틀
post-order: 4                               # (내 커스텀 변수) 같은 카테고리 내 정렬 순서
comments: true
---

# Manifold hypothesis

'세상의 모든 데이터는 고차원 공간 안의 저차원 매니폴드 위에 존재한다'는 딥러닝의 근본적인 가정이다.
딥러닝은 데이터를 가장 잘 설명하는 곡면을 찾는다. 즉 데이터들이 어떤 구조를 가진 부드러운 곡면 위에 존재하므로 딥러닝을 사용하면 그 근서적인 구조를 데이터로부터 학습해 낼 수 있다는 것이다.
이 구조는 간단하고 저차원이어서 컴퓨터가 처리할 수 있다. 또한, 메니폴드는 연속적이기 때문에 데이터들을 보간(interpolation) 하여 그 구조를 근사적으로 만들어낼 수 있다.

예시로는 찌그러진 종이를 펴는 과정을 딥러닝에 자주 비유한다.
딥러닝에서 각 연산과 활성함수를 통해 찌그러진 manifold를 평면으로 펼쳐 데이터 클래스 간 경계를 찾기 편하게 만든다는 것이다.

<p/>
  
# Training
딥러닝 모델을 학습시키는 과정이다. 주로 forward pass, back propagation으로 이루어져 있으며, 모델을 테스트 할때는 forward pass만 수행하면 된다.
학습시에는 데이터로부터 표현을 학습하기 위해 예측과 개선을 반복한다.

Input을 모델에 넣어 예측값을 얻으면, 실제값 (레이블) 과 차이를 계산하고, 역전파 알고리즘으로 모델의 가중치를 조절한다.


# Back propagation
역전파라고도 하며, 예측값과 실제값의 차이를 계산하는 손실함수를 토대로 gradient를 계산한다.
딥러닝 모델의 가중치와 input 값, true value 를 알고 있으므로, 연쇄법칙(chain rule)을 이용하면 gradient를 구할 수 있다.

grad(loss,w_i) 를 구하면 각각의 가중치를 얼마나 더해야 가장 가파르게 최적의 가중치로 수렴할 수 있는지 알 수 있다.

# Overfitting
딥러닝으로 문제를 해결할때는 최적화(optimization)와 일반화(generalization)의 trade-off에 맞닥뜨리게 된다.
딥러닝은 훈련 과정에 사용한 데이터만으로 학습하기 때문에 처음보는 데이터가 있을 수 밖에 없다.
처음 보는 데이터도 얼마나 잘 예측하는가가 모델의 일반화 성능, 훈련 과정에서 얼마나 잘 예측하느냐가 최적화 성능을 결정한다.
문제는 훈련 데이터가 해결하고자 하는 문제의 모든 데이터를 대표하지 못한다는 것이다.
따라서 훈련 데이터에 너무 최적화 시켜서 데이터에 따른 정답을 다 외워버릴 정도로 최적화하면 처음보는 데이터는 제대로 예측하지 못할것이고, 이는 일반화 성능저하로 이어진다.

적절한 훈련정도를 찾기 위해 훈련데이터의 일부를 검증 데이터로 분리하여 마치 처음보는 데이터인냥 모델의 일반화 성능을 검증하는데 사용할 수 있다.

최적적합은 최적화와 일반화가 모두 잘 되었을때이다. 최적적합에 도달하려면 훈련손실은 계속 감소하는데 검증손실이 증가해야 한다. 
즉, 훈련데이터에 과도하게 학습되어 봐야 언제 멈춰야 하는지 알 수 있고, 이때 과대적합된 상태를 overfitting되었다고 한다.
 





