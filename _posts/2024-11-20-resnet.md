---
layout: post
title: "ResNet"  # 페이지 타이틀
post-order: 2                                 # (내 커스텀 변수) 같은 카테고리 내 정렬 순서
comments: true
categories:
  - paperReview
---


다음 논문을 살펴본다.

[Deep Residual Learning for Image Recognition][paperlink]

[paperlink]:https://arxiv.org/abs/1512.03385

# summary
기존 신경망은 layer depth가 증가할수록 degradation problem이 발생한다. 
overfitting은 train error가 감소해도 test/validation error가 더이상 감소하지 않거나 증가하는 현상의 원인이다.
반면 degradation은 더 많은 layer를 추가한 모델이 수렴하였을때 기존의 모델보다 train error가 더 높거나 낮아지지 않는 문제이다.
즉 모델의 역량은 더 좋아져서 성능이 더 좋아야 하는데 오히려 나빠진 경우이다.
이는 모델의 깊이가 깊어질수록 최적화가 어려워지기 때문이다.

그러나, 새로 추가된 layer가 identity layer라면 성능이 더 나빠져서는 안되지 않겠는가?
이 아이디어에서 창안된게 resnet이다.

<p align="center">
  <img src="https://github.com/user-attachments/assets/e6780eed-a37f-48be-8a90-cb40836223ec" width="50%" height="50%" alt="default" />
</p>

2~3 layer마다 short circuit을 두어 뉴럴넷이 H(x)라는 목적함수를 바로 학습하지 않고 H(x)-x를 학습하게 만든 것이다.
이 형태가 residual form 이라 resnet이라고 이름을 붙였다.

resnet을 사용하면 수렴도 더 빨라지며, dropout을 사용하지 않아도 degradation problem을 해결할 수 있다.
resnet자체가 새로운 가중치를 추가하는 것이 아니기에 시간복잡도도 유지된다!

다만, resnet이 overfitting을 해결해 주는것은 아니기에, 단순한 문제에 너무 깊은 network를 사용하지는 말자.

# discussion

누구든지 이해할 수 있는 수학적 배경에, 아주 간단한 구조만 추가해도 성능이 개선된다는 점에서 정말 좋은 논문인것 같다.
deep layer를 가진 모델중 resnet을 사용하지 않는 모델이 적을 정도로 많이 사용될 정도로 큰 영향을 준 논문이다. 
이런 논문을 쓰고싶다!

