---
layout: single # post
classes: wide
title: "Transformer"  # 페이지 타이틀
post-order: 4                               # (내 커스텀 변수) 같은 카테고리 내 정렬 순서
use_math: true
categories:
  - paperreview
---

[Attention Is All You Need][paperlink]

[paperlink]:https://arxiv.org/abs/1706.03762

위 논문을 살펴본다. 3 blue 1 brown의 transformer 영상을 참조하였다.

# summary

신경망을 사용하는 언어모델은 두가지 핵심 단계를 거친다.   
먼저, 단어를 신경망이 처리할 수 있는 숫자로 바꿔주는 것이다. 단어의 의미 관계를 더 잘 표현하기 위해 벡터로 만들면, 단어간의 의미를 벡터 합과 같은 연산으로 더하여 의미있는 단어로 매핑되도록 만들 수 있다는 장점이 있다. 즉, 벡터는 단어를 더 압축적으로, 잘 표현할 수 있다.    
두번째는 각 단어 다음에 올 확률이 가장 높은 단어를 선택하는 것이다. 약간의 유동성을 부여하기 위해 해당 확률로 단어를 선택할수도 있다. 이러면 결과의 다양성이 증가하지만, 같은 초기값을 주더라도 다른 값이 나올 수 있어 일관성이 줄어든다.     


여기서, 트랜스포머는 self attention layer와 MLP layer 두가지 개념을 추가하여 성능을 더욱 끌어올렸다.


self attention은 주어진 문자열에서 문맥을 분석한다. 어떤 단어가 다른 단어랑 얼만큼 연관이 있는지 계산하여 각 단어의 관계를 바탕으로 단어벡터들을 수정한다. 
다른 단어와 연관된 정도를 계산하고 얻는 벡터가 Q, K, V 이다. Q는 문맥 정보에 대한 질의, K는 문맥정보에 대한 일종의 답변, V는 K에 해당하는 가중치 값으로 생각할 수 있다. 
아래 식을 통해 계산한 값이 attention이며, 이를 원래 단어 벡터에 더하여 문맥정보를 반영한다.

이어지는 MLP은 명제 같은 의미정보를 학습하며, 단어가 통과할때 각 의미들과 연관된 정도를 계산하여 다음 단어를 예측한다.
학습된 여러개의 의미 행렬에 단어를 각각 곱하여 의미적으로 가까운 정도를 계산하고, 활성화 함수를 거친 뒤, 각 의미에 대응되는 값 행렬을 곱하여 단어에 정보를 추가한다.

이 과정을 여러번 거친 후 다음에 올 단어를 최종적으로 예측한다.


여기서 self attention을 병렬로 여러개 수행하도록 만든게 Multi head attention이다. head마다 학습이 다르게 되어, 어떤 것은 명사에 attention을 높게 부여하고, 다른것은 동사에 높게 부여하는 등 차이가 있다.      
즉, head개수가 늘어나면 입력 문장을 다각도로 분석할 수 있게 된다. 하지만 너무 많으면 성능이 오히려 떨어져서, transformer에는 8개로 제한하였다. 


# discussion
신경망을 사용하는 언어모델의 성능을 혁신적으로 개선한 모델로써, 언어모델에 국한되지 않고 다양한 분야에 적용되었다. 사람이 문장을 해석할 때 특정 부분에 '집중' 하는 것을 모티브로 만들어 졌다. 
이 논문처럼 우리가 알게 모르게 사용하는 습관이나 방법을 눈치채면, 이를 인공지능 모델에 구현해 보는것도 좋을 것 같다.  



