---
layout: single # post
title: "GAN"  # 페이지 타이틀
post-order: 4                               # (내 커스텀 변수) 같은 카테고리 내 정렬 순서
# comments: true
categories:
  - paperreview
---

다음 논문을 살펴본다.

[Generative Adversarial Networks][paperlink]

[paperlink]:https://arxiv.org/abs/1406.2661

# summary
Back propagation과 dropout만을 사용한 하나의 신경망으로는 진짜같은 이미지를 생성하기 어렵다. 
진짜같다의 기준을 간단한 손실함수로 만들 수 있겠는가?

GAN은 노이즈로부터 이미지를 생성하는 신경망과, 훈련 데이터인지 생성된 이미지인지 판별하는 신경망 두개를 이용하여 이 문제를 우회한다.
이 신경망들이 서로 대립한다는 의미에서 두 신경망을 adversarial network라고 한다. 여기에 생성모델이라는 의미를 붙여서 Generative Adversarial Network(GAN)이라고 한다.

생성자는 랜덤 노이즈(z)를 신경망을 거쳐 그럴싸한 이미지(G(z))로 만든다. 판별자를 잘 속이는가로 손실함수를 정의한다.

손실함수 적기

판별자는 생성된 이미지와 훈련 이미지 중 랜덤하게 뽑아서, 이 이미지가 훈련 데이터에서 나왔을 확률을 계산한다. 훈련 데이터를 올바르게 판정함과 동시에 생성된 데이터를 가짜라고 잘 판정해야 하므로 다음과 같이 손실함수를 정의한다.

판별자는 훈련 데이터를 올바르게 훈련 데이터라고 예측할 수 있어야 하고, 생성자는 생성된 데이터를 판별자가 훈련 데이터로 착가각하도록 속여야 한다. 즉 D(G(z))를 최대화 해야 한다.
이를 손실함수로 나타내면 다음과 같은 간단한 식으로 만들 수 있다.

V D G 함수 적기

이론대로 모델이 수렴했을때, 생성자는 훈련 이미지 분포를 잘 학습한 상태이고, 판별자는 만들어진 이미지가 생성된건지 훈련 데이터인지 알 수 없으니 훈련 데이터라고 예측할 확률이 1/2이 된다.

모델을 훈련할때는 판별자와 생성자를 번갈아가며 학습시킨다.

1. 훈련 데이터와 생성된 데이터를 같은 개수만큼 뽑고 판별자가 수렴할때까지 학습시킨다.
2. 학습된 판별자를 속이도록 생성자를 학습시킨다.
3. 수렴할때까지 1,2를 반복한다.

현실적으로 1은 시간이 너무 많이 걸리기에 실제 훈련할때는 임의로 k번 학습시키도록 타협한다.

위와 같은 알고리즘으로 학습을 진행한다면 생성자의 분포가 훈련 데이터의 분포로 수렴함을 증명할 수 있다.
하지만 현실적인 타협으로 인해 생성자가 만든 분포가 진동하거나 mode collapse가 발생하기도 한다.



# discussion
GAN은 심플한 아이디어로도 빠르고 좋은 성능을 낼 수 있다는 점에서 매력적이다. 
그러나, generator와 discriminator가 골고루 학습되어야 한다는 치명적인 단점이 있다.
예를들어 생성자가 너무 진짜처럼 생성을 잘하는 반면 판별자가 학습이 덜 되었다고 하자. 판별자는 생성자가 만든 이미지를 대부분 진짜라고 판별할 것이고, 이로인해 생성자는 더이상 성능이 개선되지 않으며, 판별자도 마찬가지이다. 

이러한 문제점을 피하기 위한 방법들을 연구하면 안정적으로 GAN 모델을 사용할 수 있을 것이다.



