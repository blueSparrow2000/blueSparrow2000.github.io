---
layout: single # post
classes: wide
title: "RL"  # 페이지 타이틀
post-order: 4                               # (내 커스텀 변수) 같은 카테고리 내 정렬 순서
use_math: true
categories:
  - paperreview
---

이 포스트에서는 강화학습에 대한 전반적인 이해 및 소소한 팁을 적어본다.        


강화학습은 다른 인공지능 분야보다 좀 특별하다.       
이미지 인식이나 생성형 인공지능들은 패턴을 매칭하고 학습하는 반면, 강화학습은 시행착오를 통해 학습하기 때문이다.
사람이 배우는 과정과 상당히 비슷하다. 현재 상황에서 좋은 선택을 할수록 더 큰 보상을 받고, 비슷한 상황에서 해당 행동을 선택할 확률이 커지도록 행동이 강화된다.
실제로 이러한 사람의 보상체계, 중독 매커니즘을 모방하였다고 강화학습이라고 한다.
퍼셉트론을 이용한 패턴 매칭이 아닌, 학습 과정을 모방한 것이다.          
강화학습은 컴퓨터가 하기엔 모호하고, 인간을 대신하여 선택을 해야하는 문제들을 자동화 할 수 있다는 점에서 다양한 분야에 사용된다.
주로 센서를 통해 현상태를 받고 최적의 판단을 내리는 제어 문제에 사용되지만, 여러분이 자주 쓰는 chat gpt를 학습시킬때도 사용되었다.
그리고 강화학습된 컴퓨터는 많은 사람들이 좋아하는 게임을 굉장히 잘할 수 있다.
        

        
강화학습과 유전 알고리즘을 헷갈려 하는 사람이 많다.         
둘 다 시행착오를 통해 자신을 개선한다. 다만, 학습하는 것이 다르다. 
유전 알고리즘은 어떤 criteria에 가장 적절한 parameter값을 학습한다.
반면, 강화학습은 어떤 상황(state)에서 가장 적절한 선택(action) 을 선택하는 decision making process를 학습한다. 
즉 강화학습의 결과는 선택하는 방법(policy) 이다. 

        
강화학습은 결정하는 주체인 요원(agent)과 이와 상호작용하는 공간인 환경(environment)으로 이루어져 있다. 요원이 환경으로부터 정보를 얻으면(observation), 해당 정보로부터 행동을 결정하고 수행하며(action) 그 행동에 대해 환경으로부터 보상(reward)을 받는다. 여기서 관찰은 요원이 행동을 내리는 근거인 상태(state)로 나타낼 수 있다. 
행동을 결정하기 위해서 우리가 현재 어떤 상태에 있고(물 위에 있는 상황과 용암 위에 있는 상황에서 전부 수영을 할 게 아니라 각각 다른 행동을 해야 한다) 어떤 결정을 내렸을때 보상이 컸는지를 알아야 한다. 환경으로부터 얻는 정보로 이 상태를 알 수 있다. 한 상태에서 행동을 하면 다른 상태가 된다. 물 위에 떠있다가 수영을 해서 섬에 도착하면 내가 물 위가 아닌 섬 위에 있는 상태가 된다.
이 상태에서 어떤 행동을 했을때 보상이 가장 컸냐가 결정을 정하는 핵심이다. 가장 보상이 컸던 행동을 하면 된다!

          
그렇다면 좋은 결정이란 무엇일까? 더 구체적으로 알아보기 위해 유명한 기회비용 문제를 살펴보자. 
지금 눈 앞에 쿠키가 있다. (여러분은 쿠키를 좋아해서, 보이면 바로 먹고싶도록 본능이 내제되어있다) 이 공짜 쿠키를 먹으면 도파민이 분비되고 기분이 좋아진다.
근데, 다른 사람이 지금 눈 앞의 쿠키를 먹지 않고 10분을 기다리면, 눈앞의 쿠키에 더해 하나를 더 공짜로 주겠다고 말한다.
그럼 당연히 10분을 기다릴 것이다. 1초마다 당신의 본능이 눈앞의 쿠키를 먹도록 유혹해도 말이다.
그렇다면, 1000년을 기다리면 쿠키를 하나 더 준다고 말하면 어떨까? 그때가 되면 우린 이미 죽고 없을 것이다. 그래서 그냥 눈앞의 쿠키를 바로 먹는게 더 나은 선택이다.
여기서 두가지 사실을 알 수 있다. 보상은 단순히 바로 1초뒤의 보상만 생각하면 되는게 아닌, 미래에 얻을 수 있는 모든 보상의 합으로 보아야 현재 가장 좋은 선택을 할 수 있다는 것이다.
또 다른 한가지는 너무 먼 미래의 보상의 경우 그 가치가 더 작아진다는 것이다.

        
근데 강화학습의 어디에 뉴럴넷이 사용되는걸까? 바로 상태함수이다. 모든 상태에 도달하여 모든 행동을 해볼 수 있다면 가치함수를 완벽히 구할 수 있을 것이다.
근데 이럴꺼면 그냥 하드코딩하여 모든 경우를 돌아보는게 더 빠를 것이다. 하지만 많은 문제들이 수많은 상태와(심지어 연속적인) 수많은 행동이 있으며, 결정적이지 않은 상태변화(확률적인 상태변화)를 가지고 있다.
따라서, 모든 시행착오를 거쳐보지 않더라도 일반화하여 근사치를 구해보기 위해 뉴럴넷을 사용하는 것이다.





