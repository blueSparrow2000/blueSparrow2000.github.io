---
layout: single
title: "LDM"  # 페이지 타이틀
post-order: 4                               # (내 커스텀 변수) 같은 카테고리 내 정렬 순서
# comments: true
categories:
  - paperreview
---

다음 논문을 살펴본다.

[High-Resolution Image Synthesis with Latent Diffusion Models][paperlink]

[paperlink]:https://arxiv.org/abs/2112.10752

# summary
기존의 확산모델과 차이점은, autoencoder를 사용하여 이미지를 잠재공간(latent space)으로 보내는 압축을 진행한 후, 잠재공간에서 확산모델이 학습한다.
잠재공간으로 보내면 원본 데이터와 의미가 같으면서 더 낮은 차원으로 데이터를 나타낼 수 있다. 즉 이미지가 커져도 더 쉽고 빠르게 학습할 수 있다.
이 autoencoder는 한번 학습하면 다른 확산모델을 학습시킬때 또 학습시킬 필요가 없다. 재사용 가능하다!

앞선 DDPM과 동일하게 모델의 핵심 뼈대는 Unet을 사용한다. Unet은 이미지 데이터를 다루기 유리한 Inductive bias를 가지고 있다. 

Inductive bias의 의미는 이 블로그에 정리가 잘 되어있다.
[Inductive bias란?][link2]

[link2]:https://velog.io/@euisuk-chung/Inductive-Bias란

또한 뼈대가 되는 확산모델은 원래 2차원 데이터를 학습하도록 만들어졌으므로 1차원 잠재공간을 사용하는 VAE나 VQGAN보다 원본 이미지를 잘 보존한다.
Unet에 cross-attention layer를 추가하여 텍스트 정보도 같이 받아 학습시킬 수 있도록 만들었다.

이처럼 잠재공간으로 보내는 인코딩 과정을 거친 후 확산모델을 학습시키기에 Latent Diffusion Model(LDM)이라 한다. LDM의 구조 및 학습과정은 다음과 같다.

*구조 그림

1. 먼저 원본 이미지에서 약 4개마다 하나의 픽셀만 추출하는 압축과정을 거친다.
2. 인코더를 사용해 데이터를 잠재공간으로 보내고, 여기서 확산모델의 forward stage를 수행한다. 
3. 텍스트 입력은 별도의 신경망 모델(transformer 등)로 처리하여 Unet모델에 넣어 reverse stage를 수행한다.
4. 3이 끝나면 디코더를 이용해 픽셀 이미지로 재구성한다.


# discussion
