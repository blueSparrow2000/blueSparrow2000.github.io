---
layout: single
title: "LDM"  # 페이지 타이틀
post-order: 4                               # (내 커스텀 변수) 같은 카테고리 내 정렬 순서
# comments: true
categories:
  - paperreview
---

다음 논문을 살펴본다.

[High-Resolution Image Synthesis with Latent Diffusion Models][paperlink]

[paperlink]:https://arxiv.org/abs/2112.10752


# preliminary
이전 게시물에 확산모델에 대한 전반적인 소개를 해두었다. LDM은 마지막에 소개된 논문으로, 픽셀 공간에서 학습하는게 아닌 잠재공간에서 학습한다는 점애서 DDPM과 차이가 있다.

[Diffusion models][link]

[link]: https://bluesparrow2000.github.io/paperreview/diffusion/

[DDPM][link1]

[link1]: https://bluesparrow2000.github.io/paperreview/ddpm/

# summary
기존의 확산모델과 차이점은, autoencoder를 사용하여 이미지를 잠재공간(latent space)으로 보내는 압축을 진행한 후, 잠재공간에서 확산모델이 학습한다.
잠재공간으로 보내면 원본 데이터와 의미가 같으면서 더 낮은 차원으로 데이터를 나타낼 수 있다. 즉 이미지가 커져도 더 쉽고 빠르게 학습할 수 있다.
이 autoencoder는 한번 학습하면 다른 확산모델을 학습시킬때 또 학습시킬 필요가 없다. 재사용 가능하다!

앞선 DDPM과 동일하게 모델의 핵심 뼈대는 Unet을 사용한다. Unet은 이미지 데이터를 다루기 유리한 Inductive bias를 가지고 있다. 

Inductive bias의 의미는 이 블로그에 정리가 잘 되어있다.
[Inductive bias란?][link2]

[link2]:https://velog.io/@euisuk-chung/Inductive-Bias란

또한 뼈대가 되는 확산모델은 원래 2차원 데이터를 학습하도록 만들어졌으므로 1차원 잠재공간을 사용하는 VAE나 VQGAN보다 원본 이미지를 잘 보존한다.
Unet에 cross-attention layer를 추가하여 텍스트 정보도 같이 받아 학습시킬 수 있도록 만들었다.

이처럼 잠재공간으로 보내는 인코딩 과정을 거친 후 확산모델을 학습시키기에 Latent Diffusion Model(LDM)이라 한다. LDM의 구조 및 학습과정은 다음과 같다.

*구조 그림

1. 먼저 원본 이미지에서 약 4개마다 하나의 픽셀만 추출하는 압축과정을 거친다.
2. 인코더를 사용해 데이터를 잠재공간으로 보내고, 여기서 확산모델의 forward stage를 수행한다. 
3. 텍스트 입력은 별도의 신경망 모델(transformer 등)로 처리하여 Unet모델에 넣어 reverse stage를 수행한다.
4. 3이 끝나면 디코더를 이용해 픽셀 이미지로 재구성한다.


# discussion

LDM은 유명한 Stable diffusion의 핵심 모델이다. 그만큼 이미지 생성성능이 좋다.
하지만 여전히 GAN같은 생성모델보다는 느리고, 픽셀 단위의 정확한 이미지를 생성하기는 어렵다.

데이터에 개인 정보가 섞여 들어갈 경우 생성된 이미지에 나타날 수 있는 문제점도 있다.

또한, 다른 딥러닝 모델처럼 데이터에 내제된 편향성을 학습하게 될 수 있다. 예를 들어 고양이가 99%인 데이터로 강아지를 생성해 달라하면 고양이같은 무언가를 생성할 수 있다.

요즘 각광받는 SORA는 Unet이 아닌 Diffusion Transformer구조를 사용한다. 다른 포스트에서 LDM과 어떤 차이점이 있는지 살펴본다.
